{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download S3 Logs to DataFrame\n",
    "\n",
    "This notebook downloads all JSON log files from the `staging-eks-otel-logging` S3 bucket and loads them into a pandas DataFrame.\n",
    "\n",
    "The files are organized in a partitioned structure:\n",
    "```\n",
    "logs/\n",
    "  year=2025/\n",
    "    month=12/\n",
    "      day=04/\n",
    "        hour=15/\n",
    "          minute=21/\n",
    "            logs_*.json\n",
    "```\n",
    "\n",
    "## Setup\n",
    "\n",
    "Before running this notebook, ensure you have the required dependencies installed:\n",
    "\n",
    "```bash\n",
    "uv sync --group dev\n",
    "```\n",
    "\n",
    "Also ensure your AWS credentials are configured (via environment variables, AWS credentials file, or IAM role)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Any\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "# Configure AWS client\n",
    "# Credentials can be set via:\n",
    "# - AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables\n",
    "# - AWS credentials file (~/.aws/credentials)\n",
    "# - IAM role (if running on EC2/ECS)\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "BUCKET_NAME = 'staging-eks-otel-logging'\n",
    "PREFIX = 'logs/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List all JSON files in the bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listing all JSON files in bucket...\n",
      "Found 34 JSON files\n",
      "Sample files (first 5):\n",
      "  - logs/year=2025/month=12/day=03/hour=18/minute=15/logs_217695606.json\n",
      "  - logs/year=2025/month=12/day=03/hour=18/minute=15/logs_364109350.json\n",
      "  - logs/year=2025/month=12/day=03/hour=18/minute=15/logs_480615163.json\n",
      "  - logs/year=2025/month=12/day=03/hour=18/minute=15/logs_481319329.json\n",
      "  - logs/year=2025/month=12/day=03/hour=18/minute=19/logs_107713162.json\n"
     ]
    }
   ],
   "source": [
    "def list_all_json_files(bucket: str, prefix: str) -> list[str]:\n",
    "    \"\"\"List all JSON files in the S3 bucket with the given prefix.\"\"\"\n",
    "    json_files = []\n",
    "    paginator = s3_client.get_paginator('list_objects_v2')\n",
    "\n",
    "    for page in paginator.paginate(Bucket=bucket, Prefix=prefix):\n",
    "        if 'Contents' in page:\n",
    "            for obj in page['Contents']:\n",
    "                key = obj['Key']\n",
    "                if key.endswith('.json'):\n",
    "                    json_files.append(key)\n",
    "\n",
    "    return json_files\n",
    "\n",
    "# List all JSON files\n",
    "print(\"Listing all JSON files in bucket...\")\n",
    "all_files = list_all_json_files(BUCKET_NAME, PREFIX)\n",
    "print(f\"Found {len(all_files)} JSON files\")\n",
    "print(\"Sample files (first 5):\")\n",
    "for f in all_files[:5]:\n",
    "    print(f\"  - {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and parse JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and parsing 34 files...\n",
      "Successfully downloaded 34 files\n",
      "Total log entries: 47\n"
     ]
    }
   ],
   "source": [
    "def download_and_parse_json(bucket: str, key: str) -> list[dict[str, Any]]:  # noqa: C901\n",
    "    \"\"\"Download a JSON file from S3 and parse it, extracting individual logRecords.\n",
    "\n",
    "    Returns a list of log record dictionaries. The function looks for:\n",
    "    - 'resourceLogs' array (OpenTelemetry format)\n",
    "    - 'logRecords' array (AWS CloudWatch format)\n",
    "    - Direct array of log entries\n",
    "    - Single log entry object\n",
    "\n",
    "    Each log record will have metadata added (S3 key, bucket, partition info).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "        content = response['Body'].read().decode('utf-8')\n",
    "        parsed = json.loads(content)\n",
    "\n",
    "        # Extract partition information from the key path\n",
    "        # Format: logs/year=2025/month=12/day=04/hour=15/minute=21/logs_*.json\n",
    "        partition_info = {}\n",
    "        parts = key.split('/')\n",
    "        for part in parts:\n",
    "            if '=' in part:\n",
    "                key_val = part.split('=')\n",
    "                if len(key_val) == 2:\n",
    "                    partition_key = key_val[0]\n",
    "                    partition_value = key_val[1]\n",
    "                    partition_info[f'_partition_{partition_key}'] = partition_value\n",
    "\n",
    "        # Extract log records from various possible structures\n",
    "        log_records = []\n",
    "\n",
    "        if isinstance(parsed, list):\n",
    "            # If the root is an array, treat each element as a log record\n",
    "            log_records = parsed\n",
    "        elif isinstance(parsed, dict):\n",
    "            # Check for common log record array fields\n",
    "            if 'resourceLogs' in parsed:\n",
    "                # OpenTelemetry format: extract from resourceLogs array\n",
    "                for resource_log in parsed.get('resourceLogs', []):\n",
    "                    # Extract scopeLogs which contain the actual log records\n",
    "                    for scope_log in resource_log.get('scopeLogs', []):\n",
    "                        log_records.extend(scope_log.get('logRecords', []))\n",
    "            elif 'logRecords' in parsed:\n",
    "                # AWS CloudWatch format\n",
    "                log_records = parsed['logRecords']\n",
    "            elif 'logs' in parsed:\n",
    "                # Alternative format\n",
    "                log_records = parsed['logs']\n",
    "            else:\n",
    "                # If no recognized structure, treat the whole object as a single log record\n",
    "                log_records = [parsed]\n",
    "\n",
    "        # Add metadata to each log record\n",
    "        result = []\n",
    "        for record in log_records:\n",
    "            if isinstance(record, dict):\n",
    "                # Create a copy to avoid modifying the original\n",
    "                enriched_record = record.copy()\n",
    "\n",
    "                # Add S3 metadata\n",
    "                enriched_record['_s3_key'] = key\n",
    "                enriched_record['_s3_bucket'] = bucket\n",
    "\n",
    "                # Add partition information\n",
    "                enriched_record.update(partition_info)\n",
    "\n",
    "                result.append(enriched_record)\n",
    "\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {key}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Download and parse all files\n",
    "print(f\"Downloading and parsing {len(all_files)} files...\")\n",
    "all_data = []\n",
    "\n",
    "for i, file_key in enumerate(all_files, 1):\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Processing file {i}/{len(all_files)}...\")\n",
    "\n",
    "    entries = download_and_parse_json(BUCKET_NAME, file_key)\n",
    "    all_data.extend(entries)\n",
    "\n",
    "print(f\"Successfully downloaded {len(all_files)} files\")\n",
    "print(f\"Total log entries: {len(all_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame shape: (47, 13)\n",
      "\n",
      "Columns: ['timeUnixNano', 'observedTimeUnixNano', 'severityNumber', 'severityText', 'body', '_s3_key', '_s3_bucket', '_partition_year', '_partition_month', '_partition_day', '_partition_hour', '_partition_minute', 'attributes']\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timeUnixNano</th>\n",
       "      <th>observedTimeUnixNano</th>\n",
       "      <th>severityNumber</th>\n",
       "      <th>severityText</th>\n",
       "      <th>body</th>\n",
       "      <th>_s3_key</th>\n",
       "      <th>_s3_bucket</th>\n",
       "      <th>_partition_year</th>\n",
       "      <th>_partition_month</th>\n",
       "      <th>_partition_day</th>\n",
       "      <th>_partition_hour</th>\n",
       "      <th>_partition_minute</th>\n",
       "      <th>attributes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1717016223073301200</td>\n",
       "      <td>1717016223073302800</td>\n",
       "      <td>9</td>\n",
       "      <td>INFO</td>\n",
       "      <td>{'stringValue': '[NashonServer nodeId=1] Nasho...</td>\n",
       "      <td>logs/year=2025/month=12/day=03/hour=18/minute=...</td>\n",
       "      <td>staging-eks-otel-logging</td>\n",
       "      <td>2025</td>\n",
       "      <td>12</td>\n",
       "      <td>03</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1717016223073301200</td>\n",
       "      <td>1717016223073302800</td>\n",
       "      <td>9</td>\n",
       "      <td>INFO</td>\n",
       "      <td>{'stringValue': '[NashonServer nodeId=1] Nasho...</td>\n",
       "      <td>logs/year=2025/month=12/day=03/hour=18/minute=...</td>\n",
       "      <td>staging-eks-otel-logging</td>\n",
       "      <td>2025</td>\n",
       "      <td>12</td>\n",
       "      <td>03</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1717016223073301200</td>\n",
       "      <td>1717016223073302800</td>\n",
       "      <td>9</td>\n",
       "      <td>INFO</td>\n",
       "      <td>{'stringValue': '[NashonServer nodeId=1] Nasho...</td>\n",
       "      <td>logs/year=2025/month=12/day=03/hour=18/minute=...</td>\n",
       "      <td>staging-eks-otel-logging</td>\n",
       "      <td>2025</td>\n",
       "      <td>12</td>\n",
       "      <td>03</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1717016223073301200</td>\n",
       "      <td>1717016223073302800</td>\n",
       "      <td>9</td>\n",
       "      <td>INFO</td>\n",
       "      <td>{'stringValue': '[NashonServer nodeId=1] Nasho...</td>\n",
       "      <td>logs/year=2025/month=12/day=03/hour=18/minute=...</td>\n",
       "      <td>staging-eks-otel-logging</td>\n",
       "      <td>2025</td>\n",
       "      <td>12</td>\n",
       "      <td>03</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1717016223073301200</td>\n",
       "      <td>1717016223073302800</td>\n",
       "      <td>9</td>\n",
       "      <td>INFO</td>\n",
       "      <td>{'stringValue': '[NashonServer nodeId=1] Nasho...</td>\n",
       "      <td>logs/year=2025/month=12/day=03/hour=18/minute=...</td>\n",
       "      <td>staging-eks-otel-logging</td>\n",
       "      <td>2025</td>\n",
       "      <td>12</td>\n",
       "      <td>03</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          timeUnixNano observedTimeUnixNano  severityNumber severityText  \\\n",
       "0  1717016223073301200  1717016223073302800               9         INFO   \n",
       "1  1717016223073301200  1717016223073302800               9         INFO   \n",
       "2  1717016223073301200  1717016223073302800               9         INFO   \n",
       "3  1717016223073301200  1717016223073302800               9         INFO   \n",
       "4  1717016223073301200  1717016223073302800               9         INFO   \n",
       "\n",
       "                                                body  \\\n",
       "0  {'stringValue': '[NashonServer nodeId=1] Nasho...   \n",
       "1  {'stringValue': '[NashonServer nodeId=1] Nasho...   \n",
       "2  {'stringValue': '[NashonServer nodeId=1] Nasho...   \n",
       "3  {'stringValue': '[NashonServer nodeId=1] Nasho...   \n",
       "4  {'stringValue': '[NashonServer nodeId=1] Nasho...   \n",
       "\n",
       "                                             _s3_key  \\\n",
       "0  logs/year=2025/month=12/day=03/hour=18/minute=...   \n",
       "1  logs/year=2025/month=12/day=03/hour=18/minute=...   \n",
       "2  logs/year=2025/month=12/day=03/hour=18/minute=...   \n",
       "3  logs/year=2025/month=12/day=03/hour=18/minute=...   \n",
       "4  logs/year=2025/month=12/day=03/hour=18/minute=...   \n",
       "\n",
       "                 _s3_bucket _partition_year _partition_month _partition_day  \\\n",
       "0  staging-eks-otel-logging            2025               12             03   \n",
       "1  staging-eks-otel-logging            2025               12             03   \n",
       "2  staging-eks-otel-logging            2025               12             03   \n",
       "3  staging-eks-otel-logging            2025               12             03   \n",
       "4  staging-eks-otel-logging            2025               12             03   \n",
       "\n",
       "  _partition_hour _partition_minute attributes  \n",
       "0              18                15        NaN  \n",
       "1              18                15        NaN  \n",
       "2              18                15        NaN  \n",
       "3              18                15        NaN  \n",
       "4              18                15        NaN  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to DataFrame\n",
    "# If the JSON files contain nested structures, pandas will handle them appropriately\n",
    "df = pd.DataFrame(all_data)\n",
    "\n",
    "print(f\"DataFrame shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract body and attributes from log records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting body and attributes from log records...\n",
      "DataFrame shape after extraction: (47, 29)\n",
      "\n",
      "New columns added: ['log_body', 'attr_mcp.tool.name', 'attr_timestamp', 'attr_query.param.limit', 'attr_query.param.query_type', 'attr_response.success', 'attr_response.size_bytes', 'attr_response.result_count', 'attr_response.top_score', 'attr_response.query_time_ms', 'attr_response.total_results', 'attr_query.full_text', 'attr_response.results_json', 'attr_response.chunk_retrieved', 'attr_response.content_length', 'attr_response.chunk_json']\n",
      "\n",
      "Sample log bodies:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log_body</th>\n",
       "      <th>attr_mcp.tool.name</th>\n",
       "      <th>attr_timestamp</th>\n",
       "      <th>attr_query.param.limit</th>\n",
       "      <th>attr_query.param.query_type</th>\n",
       "      <th>attr_response.success</th>\n",
       "      <th>attr_response.size_bytes</th>\n",
       "      <th>attr_response.result_count</th>\n",
       "      <th>attr_response.top_score</th>\n",
       "      <th>attr_response.query_time_ms</th>\n",
       "      <th>attr_response.total_results</th>\n",
       "      <th>attr_query.full_text</th>\n",
       "      <th>attr_response.results_json</th>\n",
       "      <th>attr_response.chunk_retrieved</th>\n",
       "      <th>attr_response.content_length</th>\n",
       "      <th>attr_response.chunk_json</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[NashonServer nodeId=1] Nashon Server started</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[NashonServer nodeId=1] Nashon Server started</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[NashonServer nodeId=1] Nashon Server started</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[NashonServer nodeId=1] Nashon Server started</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[NashonServer nodeId=1] Nashon Server started</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        log_body attr_mcp.tool.name  \\\n",
       "0  [NashonServer nodeId=1] Nashon Server started                NaN   \n",
       "1  [NashonServer nodeId=1] Nashon Server started                NaN   \n",
       "2  [NashonServer nodeId=1] Nashon Server started                NaN   \n",
       "3  [NashonServer nodeId=1] Nashon Server started                NaN   \n",
       "4  [NashonServer nodeId=1] Nashon Server started                NaN   \n",
       "\n",
       "  attr_timestamp attr_query.param.limit attr_query.param.query_type  \\\n",
       "0            NaN                    NaN                         NaN   \n",
       "1            NaN                    NaN                         NaN   \n",
       "2            NaN                    NaN                         NaN   \n",
       "3            NaN                    NaN                         NaN   \n",
       "4            NaN                    NaN                         NaN   \n",
       "\n",
       "  attr_response.success attr_response.size_bytes attr_response.result_count  \\\n",
       "0                   NaN                      NaN                        NaN   \n",
       "1                   NaN                      NaN                        NaN   \n",
       "2                   NaN                      NaN                        NaN   \n",
       "3                   NaN                      NaN                        NaN   \n",
       "4                   NaN                      NaN                        NaN   \n",
       "\n",
       "   attr_response.top_score  attr_response.query_time_ms  \\\n",
       "0                      NaN                          NaN   \n",
       "1                      NaN                          NaN   \n",
       "2                      NaN                          NaN   \n",
       "3                      NaN                          NaN   \n",
       "4                      NaN                          NaN   \n",
       "\n",
       "  attr_response.total_results attr_query.full_text attr_response.results_json  \\\n",
       "0                         NaN                  NaN                        NaN   \n",
       "1                         NaN                  NaN                        NaN   \n",
       "2                         NaN                  NaN                        NaN   \n",
       "3                         NaN                  NaN                        NaN   \n",
       "4                         NaN                  NaN                        NaN   \n",
       "\n",
       "  attr_response.chunk_retrieved attr_response.content_length  \\\n",
       "0                           NaN                          NaN   \n",
       "1                           NaN                          NaN   \n",
       "2                           NaN                          NaN   \n",
       "3                           NaN                          NaN   \n",
       "4                           NaN                          NaN   \n",
       "\n",
       "  attr_response.chunk_json  \n",
       "0                      NaN  \n",
       "1                      NaN  \n",
       "2                      NaN  \n",
       "3                      NaN  \n",
       "4                      NaN  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_body(record: dict[str, Any]) -> str | None:\n",
    "    \"\"\"Extract the body/content from a log record.\"\"\"\n",
    "    if 'body' in record:\n",
    "        body = record['body']\n",
    "        # Handle different body formats\n",
    "        if isinstance(body, dict):\n",
    "            # OpenTelemetry format: body can be {'stringValue': '...'} or {'bytesValue': '...'}\n",
    "            if 'stringValue' in body:\n",
    "                return body['stringValue']\n",
    "            elif 'bytesValue' in body:\n",
    "                return body['bytesValue']\n",
    "            else:\n",
    "                return str(body)\n",
    "        else:\n",
    "            return str(body)\n",
    "    return None\n",
    "\n",
    "def extract_attributes(record: dict[str, Any]) -> dict[str, Any]:  # noqa: C901\n",
    "    \"\"\"Extract attributes from a log record and convert to flat dictionary.\"\"\"\n",
    "    attrs = {}\n",
    "\n",
    "    if 'attributes' in record:\n",
    "        attributes = record['attributes']\n",
    "\n",
    "        # Handle array format: [{'key': '...', 'value': {...}}]\n",
    "        if isinstance(attributes, list):\n",
    "            for attr in attributes:\n",
    "                if isinstance(attr, dict) and 'key' in attr:\n",
    "                    key = attr['key']\n",
    "                    value = attr.get('value', {})\n",
    "\n",
    "                    # Extract value from OpenTelemetry format\n",
    "                    if isinstance(value, dict):\n",
    "                        # Check for common value types\n",
    "                        if 'stringValue' in value:\n",
    "                            attrs[key] = value['stringValue']\n",
    "                        elif 'intValue' in value:\n",
    "                            attrs[key] = value['intValue']\n",
    "                        elif 'doubleValue' in value:\n",
    "                            attrs[key] = value['doubleValue']\n",
    "                        elif 'boolValue' in value:\n",
    "                            attrs[key] = value['boolValue']\n",
    "                        elif 'bytesValue' in value:\n",
    "                            attrs[key] = value['bytesValue']\n",
    "                        else:\n",
    "                            attrs[key] = str(value)\n",
    "                    else:\n",
    "                        attrs[key] = value\n",
    "        # Handle dictionary format\n",
    "        elif isinstance(attributes, dict):\n",
    "            attrs = attributes.copy()\n",
    "\n",
    "    return attrs\n",
    "\n",
    "# Extract body and attributes for each log record\n",
    "print(\"Extracting body and attributes from log records...\")\n",
    "\n",
    "df['log_body'] = df.apply(lambda row: extract_body(row.to_dict()), axis=1)\n",
    "\n",
    "# Extract attributes and create separate columns\n",
    "attributes_list = df.apply(lambda row: extract_attributes(row.to_dict()), axis=1)\n",
    "\n",
    "# Convert attributes dictionaries to DataFrame and merge\n",
    "if len(attributes_list) > 0:\n",
    "    attrs_df = pd.DataFrame(list(attributes_list))\n",
    "    # Prefix attribute columns to avoid conflicts\n",
    "    attrs_df.columns = [f'attr_{col}' for col in attrs_df.columns]\n",
    "\n",
    "    # Merge attributes back to main dataframe\n",
    "    df = pd.concat([df, attrs_df], axis=1)\n",
    "\n",
    "print(f\"DataFrame shape after extraction: {df.shape}\")\n",
    "new_cols = [col for col in df.columns if col.startswith('attr_') or col == 'log_body']\n",
    "print(f\"\\nNew columns added: {new_cols}\")\n",
    "print(\"\\nSample log bodies:\")\n",
    "df[['log_body'] + [col for col in df.columns if col.startswith('attr_')]][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 47 entries, 0 to 46\n",
      "Data columns (total 29 columns):\n",
      " #   Column                         Non-Null Count  Dtype  \n",
      "---  ------                         --------------  -----  \n",
      " 0   timeUnixNano                   47 non-null     object \n",
      " 1   observedTimeUnixNano           47 non-null     object \n",
      " 2   severityNumber                 47 non-null     int64  \n",
      " 3   severityText                   8 non-null      object \n",
      " 4   body                           47 non-null     object \n",
      " 5   _s3_key                        47 non-null     object \n",
      " 6   _s3_bucket                     47 non-null     object \n",
      " 7   _partition_year                47 non-null     object \n",
      " 8   _partition_month               47 non-null     object \n",
      " 9   _partition_day                 47 non-null     object \n",
      " 10  _partition_hour                47 non-null     object \n",
      " 11  _partition_minute              47 non-null     object \n",
      " 12  attributes                     39 non-null     object \n",
      " 13  log_body                       47 non-null     object \n",
      " 14  attr_mcp.tool.name             39 non-null     object \n",
      " 15  attr_timestamp                 39 non-null     object \n",
      " 16  attr_query.param.limit         32 non-null     object \n",
      " 17  attr_query.param.query_type    32 non-null     object \n",
      " 18  attr_response.success          39 non-null     object \n",
      " 19  attr_response.size_bytes       39 non-null     object \n",
      " 20  attr_response.result_count     32 non-null     object \n",
      " 21  attr_response.top_score        32 non-null     float64\n",
      " 22  attr_response.query_time_ms    32 non-null     float64\n",
      " 23  attr_response.total_results    32 non-null     object \n",
      " 24  attr_query.full_text           32 non-null     object \n",
      " 25  attr_response.results_json     32 non-null     object \n",
      " 26  attr_response.chunk_retrieved  7 non-null      object \n",
      " 27  attr_response.content_length   7 non-null      object \n",
      " 28  attr_response.chunk_json       7 non-null      object \n",
      "dtypes: float64(2), int64(1), object(26)\n",
      "memory usage: 10.8+ KB\n",
      "\n",
      "\n",
      "DataFrame description:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>severityNumber</th>\n",
       "      <th>attr_response.top_score</th>\n",
       "      <th>attr_response.query_time_ms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>47.0</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>9.0</td>\n",
       "      <td>0.636363</td>\n",
       "      <td>234.098785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.115774</td>\n",
       "      <td>367.593745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>9.0</td>\n",
       "      <td>0.032266</td>\n",
       "      <td>6.134510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>9.0</td>\n",
       "      <td>0.626680</td>\n",
       "      <td>8.340955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.0</td>\n",
       "      <td>0.657086</td>\n",
       "      <td>96.402764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9.0</td>\n",
       "      <td>0.685447</td>\n",
       "      <td>258.672476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.0</td>\n",
       "      <td>0.711202</td>\n",
       "      <td>1732.425690</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       severityNumber  attr_response.top_score  attr_response.query_time_ms\n",
       "count            47.0                32.000000                    32.000000\n",
       "mean              9.0                 0.636363                   234.098785\n",
       "std               0.0                 0.115774                   367.593745\n",
       "min               9.0                 0.032266                     6.134510\n",
       "25%               9.0                 0.626680                     8.340955\n",
       "50%               9.0                 0.657086                    96.402764\n",
       "75%               9.0                 0.685447                   258.672476\n",
       "max               9.0                 0.711202                  1732.425690"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Basic info about the dataframe\n",
    "print(\"DataFrame info:\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\n\\nDataFrame description:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Save to local file\n",
    "\n",
    "Uncomment the cell below to save the dataframe to a parquet or CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to save the dataframe\n",
    "# df.to_parquet('s3_logs.parquet', index=False)\n",
    "# df.to_csv('s3_logs.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
